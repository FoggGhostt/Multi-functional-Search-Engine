\section{Токенизация}
Сама по себе Токенизация это процесс разбиение текста на токены. В зависимости от задачи это можно делать разными 
способами, например, в качестве разделителей использовать только пробелы, или же пробелы и запятые, или же 
токенизировать текст по слогам. В моем проекте необходимо будет не только разбивать тексты различных документов на 
токены, но еще и производить с ними какие-то манипуляции для эффективного индексирования. 

\subsection{Нормализация}
Самый первых шаг это нормализация. В нее входит приведение всех символов к нижнему регистру и преобразование некоторых 
специфических букв различных языков к своим латинским аналогам: å --> a. Далее необходимо производить удаление так 
называемых стоп слов-слов, которые очень часто встречаются в языке, но не несут в себе какой-то смысл, в русском это 
местоимения, предлоги, частицы, междометия и прочее. Они будут встречаться в каждом документе и не будут при этом 
отличать его от других при поиске. После удаления стоп-слов следует провести либо стемминг, либо лемматизацию для 
уменьшения потенциального количества токенов и более качественно поиска. Разберем этот момент подробнее. 

\subsection{Стемминг}
Стемминг это обрезка слова для поиска его основы - она не всегда совпадает с 
морфологическим корнем. Основа - неизменяемая часть слова, которая выражает его. В некоторых реализациях не требуется явного 
использования алгоритмов машинного обучения, 
предполагается возможность обойтись раннее изученными алгоритмами, которые могут решить эту задачу.
Также рассмотрим варианты реализации на русском и английском языках

\subsection{Лемматизация}
Что такое Лемматизация? Это процесс приведения слова к лемме - его нормальной форме. Если говорить проще - то 
это начальная, словарная форма слова. То есть мы не всегда обрезаем слово, а например меняем его окончание/суффикс. 
Безусловно, лемматизация дает более точный по смыслу результат, но ее недостатки заключаются в том, что она требует 
намного больше ресурсов и ее использование зачастую предполагает ML. 

\subsection{Анализ готовых решений}
Существуют библиотеки для различной работы с текстом, его токенизацией, нормализацией и стеммингом. Можно выделить такие решения, как NLTK и SpaCy. 

\subsubsection{NLTK}
NLTK - Natural Language toolkit, это Python библиотека для обработки естественного языка. Она продоставляет широкую 
функциональность, начиная от базовой токенизацией и заканчивая готовыми списками стоп слов на разных языках. 
Предполагается изучение некоторых алгоритмов, которые используются в этой библиотеке и их реализация на Golang. 

\subsubsection{SpaCy}
SpaCy. Эта библиотека Python является аналогом NLTK, однако она быстрее справляется с некоторыми задачами в силу того, что написана на CPython. В процессе реализации алгоритмов стемминга также предполагается изучить их реализацию в этой библиотеке. 

