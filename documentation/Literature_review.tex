\section{Обзор литературы}

\subsection{Существующие поисковые движки}
%Включает в себя обзор источников и раскрытие темы
Если мы говорим про крупнейшие поисковые движки, то в силу закрытого исходного кода, то мы не можем знать, 
какие именно алгоритмы токенизации и ранжирования они используют, однако существует масса решение с открытым 
исходным кодом. Среди самых используемых можно выделить Elasticsearch \cite{elastic} и Solr \cite{solr}. Рассмотрим каждый из них и 
выделим, какие их части можно интегрировать в мой движок.

\subsubsection{Elasticsearch}
В первую очередь это поисковой движок для поиска по документам. Этот движок использует построение инвертированного 
индекса - индекса, который сопоставляет токенам соответствующие документы, в которых они встретились. Для этого используется 
библиотека Apache Lucene. Также в Elasticsearch может использоваться распределенный индекс - он хранится на нескольких 
серверах, что позволяет организовать горизонтальное масштабирование. Для эффективного поиска этот движок использует 
специальные метрики узлов - базы данных для хранения индекса - позволяющие распределять нагрузку по нескольким серверам. 

Приведу примеры использования в различных проектах:
\begin{itemize}
    \item {Stack Overflow использует Elasticsearch как средство для полнотекстового поиска по вопросам и 
    ответам для пользователей, а также для поиска похожих вопросов и подсказок при создании нового 
    вопроса. С помощью Elasticsearch сервис предоставляет поиск по точному совпадению (например, поиск строки кода) 
    и нечёткий поиск с большим количеством настроек.}
    \item {GitHub обеспечивает пользователей возможностями полнотекстового поиска и поиска по отдельным критериям среди 
    8 миллионов репозиториев кода благодаря Elasticsearch. Например, можно найти проект на языке Clojure, который 
    был активен в течение последнего месяца.}
\end{itemize}

\subsubsection{Solr}
Начиная с 2010 года Solr \cite{solr} и Lucene были объединены. Solr это Apache Lucene с дополнительным функционалом - поиском. 
Он использует всю ту же библиотеку Apache Lucene для построения инвертированного индекса, но добавляет алгоритм поиска 
и ранжирования результатов. 

Приведу примеры использования в различных проектах:
\begin{itemize}
    \item {Известное медиа-издание <<The Guardian>> использует Solr для обеспечения быстрого и релевантного поиска по огромному объёму новостных материалов.}
    \item {GitHub обеспечивает пользователей возможностями полнотекстового поиска и поиска по отдельным критериям среди 
    8 миллионов репозиториев кода благодаря Elasticsearch. Например, можно найти проект на языке Clojure, который 
    был активен в течение последнего месяца.}
\end{itemize}

\subsubsection{Вывод}
Из анализа существующих решений с открытым исходным кодом можно сделать вывод, что в моем поисковом движке следует 
также использовать инвертированный индекс, так как он позволяет осуществлять полнотекстовый поиск и сразу по токенам 
получать id релевантных документов. Это замедляет процесс индексирования, но кратно уменьшает время поиска.

% Что касается взаимодействия с базой данных, то в связи с тем, что мой движок будет работать с одной физической базой данных, 
% то я не буду разрабатывать алгоритмы, схожие с решениями Elasticsearch, однако безусловно стоит детально проработать
% эффективное распределение нагрузки при большом количестве запросов.
% \enlargethispage{\baselineskip}

\subsection{Токенизация}
Сама по себе Токенизация это процесс разбиение текста на токены. В зависимости от задачи это можно делать разными 
способами, например, в качестве разделителей использовать только пробелы, или же пробелы и запятые, или же 
токенизировать текст по слогам. В моем проекте необходимо будет не только разбивать тексты различных документов на 
токены, но еще и производить с ними какие-то манипуляции для эффективного индексирования. 

\subsubsection{Нормализация}
Самый первых шаг это нормализация. В нее входит приведение всех символов к нижнему регистру и преобразование некоторых 
специфических букв различных языков к своим латинским аналогам: å --> a. Далее необходимо производить удаление так 
называемых стоп слов-слов, которые очень часто встречаются в языке, но не несут в себе какой-то смысл, в русском это 
местоимения, предлоги, частицы, междометия и прочее. Они будут встречаться в каждом документе и не будут при этом 
отличать его от других при поиске. После удаления стоп-слов следует провести либо стемминг, либо лемматизацию для 
уменьшения потенциального количества токенов и более качественно поиска. Разберем этот момент подробнее. 

\subsubsection{Стемминг}
Стемминг \cite{stem1} это обрезка слова для поиска его основы - она не всегда совпадает с 
морфологическим корнем. Основа - неизменяемая часть слова, которая выражает его. В некоторых реализациях не требуется явного 
использования алгоритмов машинного обучения, 
предполагается возможность обойтись раннее изученными алгоритмами, которые могут решить эту задачу.
Также рассмотрим варианты реализации на русском и английском языках

\subsubsection{Лемматизация}
Лемматизация \cite{lemma1} \cite{lemmacomp} это процесс приведения слова к лемме - его нормальной форме. Если говорить проще - то 
это начальная, словарная форма слова. То есть мы не всегда обрезаем слово, а например меняем его окончание/суффикс. 
Безусловно, лемматизация дает более точный по смыслу результат, но ее недостатки заключаются в том, что она требует 
намного больше ресурсов и ее использование зачастую предполагает ML. 

\subsection{Индексирование}
После стемминга или лемматизации наш документ превращается в набор токенов, каждый из которых был 
нормализован и сокращен с помощью специальных алгоритмов. Построение инвертированного индекса заключается 
в сопоставлении токену документов, в которых встретился этот токен. Для эффективной выдачи результатов 
также необходимо отсортировать документы по убыванию количества вхождения в них определенного токена, чтобы при
необходимости можно было быстро искать топ-k релевантных документов по поисковому запросу. 

\subsection{Поиск}
Существует две базовые метрики оценки релевантности документов в информационном поиске — TF-IDF \cite{ctan2}, \cite{ctan1}, \cite{ctan} и 
семейство метрик BM25 \cite{bm1} \cite{bm2}, которые по 
своей сути являются усовершенствованными версиями первой. Остальные алгоритмы в большинстве своем используют дополнительные механизмы 
для повышения точности ранжирования. 
Современные системы информационного поиска часто комбинируют статистические признаки, подобные тем, что используются в TF-IDF, с
более сложными подходами, учитывающими контекст запроса и поведения пользователей. Рассмотрим их. 

\subsubsection{TF-IDF}
TF-IDF - статистическая мера, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов. 
Вес некоторого токена пропорционален частоте употребления этого токена в документе и обратно пропорционален частоте употребления токена 
во всех документах коллекции. Она состоит из двух частей - TF и IDF. TF - term frequency, частота 
вхождения слова в документ, формула для ее расчета:
\begin{align*}
    \mathrm{TF}(t, d) := \frac{n_t}{\sum_k n_k}
\end{align*}
где $n_i$ число вхождений токена i в документ d.

IDF - inverse document frequency, формула для ее расчета:
\begin{align*}
    \mathrm{IDF(t, D)} = \ln\left(\frac{|D|}{|\{d_i \mm | \mm t \in d_i\}|}\right)
\end{align*}
где $D$ - набор документов, а знаменатель - мощность множества документов, в которые входит
токен t. 

Таким образом, $\mathrm{TF|IDF}(t, d, D) = \mathrm{TF}(t, d) \cdot \mathrm{IDF}(t, D)$.
Эта метрика оценивает важность вхождения токена в документ, причем если какой-то токен встречается во всех документах, 
то TF-IDF для этого токена будет равна нулю.

Поиск осуществляется следуюшим образом - для поступающего поискового запроса $Q$ формируется
вектор TF-IDF, TF которого рассчитывается на основе его собственных токенов, а IDF - на 
основе вхождения токенов во всю коллекцию документов. Далее для всех файлов 
формируется матрица TF-IDF, строки которой являются векторами TF-IDF соответствующих документов, а столбцы
соответсвуют токенам запроса. Документы упорядочиваются с помощью сравнения косинуса угла между вектором запроса и векторами 
документов. 

\subsubsection{BM25}
BM25 - метрика, также основанная на модели <<мешка слов>>, которая оценивает релевантность документа запросу, исходя из встречаемости токенов 
запроса в документе без учёта их порядка. Эта функция представляет собой семейство методов с настраиваемыми 
параметрами, и одна из распространенных форм определяется следующим образом:

Пусть дан запрос \( Q \), состоящий из токенов \( q_1, q_2, \dots, q_n \). Тогда оценка релевантности документа \( D \) вычисляется по формуле:
\begin{align*}
    \mathrm{score}(D, Q) = \sum_{i=1}^{n} \mathrm{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \Bigl(1 - b + b \cdot \frac{|D|}{\mathrm{avgdl}}\Bigr)}
\end{align*}
где:
\begin{itemize}
    \item \( f(q_i, D) \) --- частота слова \( q_i \) в документе \( D \) (TF);
    \item \( |D| \) --- длина документа (количество слов);
    \item \( \mathrm{avgdl} \) --- средняя длина документа в коллекции;
    \item \( k_1 \) и \( b \) --- настраиваемые коэффициенты.
\end{itemize}

\( \mathrm{TF} \) и \( \mathrm{IDF}(q_i) \) определяются также, как и в случае TF-IDF.

Таким образом, BM25 учитывает как частоту появления слова в документе, так и его распространённость в коллекции, корректируя оценку за счет нормировки по длине документа и введения настраиваемых параметров.




\subsection{Анализ готовых решений}
Существуют библиотеки для различной работы с текстом, его токенизацией, нормализацией и стеммингом. Можно выделить такие решения, как NLTK и SpaCy. 

\subsubsection{NLTK}
NLTK - Natural Language toolkit \cite{nltk}, это Python библиотека для обработки естественного языка. Она продоставляет широкую 
функциональность, начиная от базовой токенизацией и заканчивая готовыми списками стоп слов на разных языках. 
Предполагается изучение некоторых алгоритмов, которые используются в этой библиотеке и их реализация на Golang. 

Среди минусов можно отметить:
\begin{itemize}
    \item NLTK не оптимизирован для высокопроизводительных систем — в задачах, где требуется быстрый анализ 
    больших объёмов данных, он может оказаться медленным.
    \item Построение эффективного обратного индекса является основой любой поисковой системы. NLTK предоставляет базовые 
    инструменты для токенизации и обработки текста, но не имеет встроенных оптимизированных решений для создания и 
    управления поисковыми индексами. 
\end{itemize}

\subsubsection{SpaCy}
SpaCy \cite{spacy} - эта библиотека Python является аналогом NLTK, однако она быстрее справляется с некоторыми 
задачами в силу того, что написана на CPython. В процессе реализации алгоритмов стемминга также 
предполагается изучить их реализацию в этой библиотеке. 

Среди минусов можно отметить:
\begin{itemize}
    \item spaCy использует особую систему хеширования строк для экономии памяти \cite{spacy_troubles}. Все строки кодируются в хеш-значения, и внутренне spaCy оперирует только этими значениями. Хотя это эффективно для экономии памяти, такая система имеет ограничения:
    \begin{enumerate}
        \item Хеши невозможно преобразовать обратно в строки без доступа к словарю (Vocab)
        \item Необходимо обеспечить, чтобы все объекты имели доступ к одному и тому же словарю, иначе spaCy не сможет найти нужные строки
    \end{enumerate}
    Эти особенности могут создать проблемы при распределенном индексировании и поиске, когда разные части системы работают с разными экземплярами словаря.
\end{itemize}

