\section{Реализация}
\subsection{Исходный код}
С исходным кодом проекта можно ознакомиться по ссылке: 
\href{https://github.com/FoggGhostt/Multi-functional-Search-Engine}{Multi-functional-Search-Engine}
\subsection{Структура проекта}
Приведу структуру моего проекта:
\tiny 
\begin{verbatim}
    .
    |-- Dockerfile
    |-- build
    |-- config.yaml
    |-- docker-compose.yml
    |-- frontend
    |   |-- README.md
    |   |-- index.html
    |   |-- jsconfig.json
    |   |-- package-lock.json
    |   |-- package.json
    |   |-- public
    |   |-- src
    |   |   |-- App.vue
    |   |   |-- assets
    |   |   |-- components
    |   |   |   |-- Background.vue
    |   |   |   |-- FileUploadButton.vue
    |   |   |   |-- SearchBar.vue
    |   |   |   \-- SearchResults.vue
    |   |   \-- main.js
    |   |-- vite.config.js
    |   \-- vue.config.js
    |-- go.mod
    |-- go.sum
    |-- main.go
    |-- pkg
    |   |-- API
    |   |   \-- search_API.go
    |   |-- config
    |   |   \-- config.go
    |   |-- indexer
    |   |   |-- indexer.go
    |   |   \-- indexer_test.go
    |   |-- models
    |   |   \-- index.go
    |   |-- mongodb
    |   |   |-- db.go
    |   |   \-- query.go
    |   |-- parser
    |   |   |-- file_readers.go
    |   |   |-- parser.go
    |   |   |-- stop_words.go
    |   |   |-- tokenizer.go
    |   |   |-- parser_test.go
    |   |   \-- utils
    |   |       |-- english_stop_words.txt
    |   |       \-- russian_stop_words.txt
    |   \-- search
    |       |-- search.go
    |       |-- search_tokenizer.go
    |       |-- tf-idf.go
    |       \-- search_test.go
    \-- search_test
\end{verbatim}

\newpage
\normalsize
\subsection{Архитектура проекта}
Приведу архитектуру моего приложения:
\begin{figure}[htbp]
    \centering
    % Масштабирование схемы на всю ширину страницы:
    \resizebox{\textwidth}{!}{%
      \begin{tikzpicture}[node distance=4cm, auto, >=Stealth]
        % Определение стилей узлов и стрелок
        \tikzstyle{block} = [rectangle, draw, fill=blue!20,
                              text width=6em, text centered,
                              rounded corners, minimum height=2em]
        \tikzstyle{line} = [draw, -stealth, thick]
  
        % Процесс загрузки файла:
        \node[block] (user1) {Пользователь};
        \node[block, right=of user1] (front1) {Фронтенд (JS)};
        \node[block, right=of front1] (server1) {Сервер (Go)};
        \node[block, right=of server1] (db1) {База данных};
        \node[block, right=of db1] (server3) {Сервер (Go)};
  
        \draw[line] (user1) -- node[align=center]{Нажатие кнопки\\загрузки файлов} (front1);
        \draw[line] (front1) -- node[align=center]{Получение сервером\\данных} (server1);
        \draw[line] (server1) -- node[align=center]{Индексирование полученных\\документов в БД} (db1);
        \draw[line] (db1) -- node[align=center]{Ответ об\\успехе/неуспехе} (server3);
        \draw[line] (server3) |- ++(0,-2) -| node[pos=0.25, below, align=center]{Ответ об\\успехе/неуспехе} (user1);
  
        % Процесс поиска:
        \node[block, below=5cm of user1] (user2) {Пользователь};
        \node[block, right=of user2] (front2) {Фронтенд (JS)};
        \node[block, right=of front2] (server2) {Сервер (Go)};
        \node[block, right=of server2] (db2) {База данных};
  
        \draw[line] (user2) -- node[align=center]{Поиск\\(ввод запроса)} (front2);
        \draw[line] (front2) -- node[align=center]{Получение сервером\\данных} (server2);
        \draw[line] (server2) -- node[align=center]{Индексирование запроса,\\запрос в БД} (db2);
        \draw[line] (db2) -- node[align=center]{Результаты поиска} (server2);
        \draw[line] (server2) -- node[align=center]{Ранжирование\\результатов} (front2);
        \draw[line] (front2) -- node[align=center]{Выдача результата\\поиска} (user2);
      \end{tikzpicture}
    }
    \caption*{Схема архитектуры поискового движка}
    \label{fig:search_engine_architecture}
  \end{figure}  

\subsection{Parser package}
Как мы уже знаем, токенизация - это процесс разбиения текста на токены. 
Токен же представляет собой нормализованную строку. Я опишу, как происходит 
разбиение на токены txt-файла.
PDF документы в свою очередь обрабатываются с помощью преобразования в txt. 

Основным функционалом данного пакета является чтение файла и его последующая обработка, заключающаяся в токенизации. 
В связи с тем, что размер файла может превышать размер оперативной памяти, необходимо 
читать файл поблочно, причем для повышения скорости обработки документов используются горутины - благодаря
чему обработка текста происходит конкурентно, а следовательно - быстрее.  
\par
Каждая горутина обрабатывает блок файла фиклированной длины. Принято решение, что оптимальный размер блока с учетом скорости 
последующего стемминга составляет $2^{20}$ байт, что позволяет не создавать слишком много горутин, но при этом 
ускоряет обработку файла. После чтения определенного блока файла из памяти каждая горутина вызыват функцию токенизации. 

\subsubsection*{Пошаговое описание токенизации документа}
Токенизация файла происходит в несколько шагов:
\begin{itemize}
    \item {Нормализация блока текста}
    \item {Разбиение на токены}
    \item {Удаление стоп-слов}
    \item {Стемминг токенов}
    \item {Подсчет вхождения каждого токена в документ}
\end{itemize}
\par \vspace{10pt}
Нормализация просходит с помощью функции ToLower пакета strings. Разбиение на токены же происходит по 
пробелам, запятым и всем остальным знакам препинания.
\par
Подробно опишу, как происходит обработка наличия стоп-слов. Напомню, что стоп-слова - это такие слова, которые 
встречаются в большинстве документов и не несут в себе никакой дополнительной информации. Если не производить 
их отсеивание, то индекс становится кратно больше, нагружая всю систему, а качество поиска не улучнается. 
Стоп-слова хранятся в txt-файлах в специальных директориях проекта, при запуске исполняемого файла проекта
происходит чтение этих файлов (Английских и Русских стоп-слов), а затем контейнер с набором этих слов сохраняется 
в глобальную переменную пакета для корректного конкурентного доступа из каждой горутины, которая обрабытывает блок текста. 
\par
Стемминг токенов осуществляется с помощью стороннего модуля, содержашего переписанный на Golang алгоритм 
Портера для Русского и Английского языков. 
%Было принято решение использовать готовый модуль под эту задачу, так как стемминг развивался в течении 
\par
Теперь опишем, как происходило построение индекса каждого документа. Сначала создавался отдельный контейнер, в котором
хранилось соответсвие вида (токен - количество вхождений). Далее каждая горутина, обработав свой блок текста и разбив его на токены, 
обращалась к специальной структуре данных и с помощью комбинации нескольких вызовов методов StoreOrLoad атомарно изменяла глобальный счетчик вхождений каждого 
токена в документ.
